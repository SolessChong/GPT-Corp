## 1
Please write a route that relays a post request to 3rd party server. It's something like this.
```
curl http://localhost:5005/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
     "model": "gpt-3.5-turbo",
     "messages": [{"role": "user", "content": "Say this is a test!"}],
     "temperature": 0.7
   }'
```
1. Route: /chat/completions
2. Write a class, in `llm.openai.py` that returns a (url, API_KEY) pair. Here url and API_KEY is hard coded in this class.
3. In this route, get API_KEY and override (may not exist) the Authorization HEADER, and relay to url + /chat/completions

## 2
Please add auth check on this relay route. Remind previous login utils.
```
    # Register Blueprints or define routes here
    # Define the login endpoint
    @app.route('/login', methods=['POST'])
    def login():
        data = request.get_json()
        username = data.get('username')
        password = data.get('password')

        # Use the check_password method from the User model
        user = User.query.filter_by(username=username).first()
        if user and user.check_password(password):
            session['user_id'] = user.id  # Store the user's id in the session
            return jsonify({"message": "Login successful"}), 200
        return jsonify({"message": "Invalid credentials"}), 401
```



curl https://ai98.vip/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-0aeCqXZcB0Zu2g0LA6EeF32a27704e26BdD0555e45Ca6bD1" \
  -d '{
     "model": "gpt-3.5-turbo",
     "messages": [{"role": "user", "content": "Say this is a test!"}],
     "temperature": 0.7
   }'
