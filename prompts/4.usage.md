## 1
Please return dict of model_name -> ratio for all the models provided by openai, in OpenAIConfig class.

basic rate: 0.001 / 1000
ratio:
    - gpt-3.5-turbo: 0.5
    - gpt-4-1106-preview: 5

```
# llm/openai.py

class OpenAIConfig:
    @staticmethod
    def get_credentials():
        # These should be replaced with environment variables or a secure method to store credentials
        URL = "https://ai98.vip/v1"
        API_KEY = "sk-ILF01mttjbDHoF7pA01aB6690d3643B98c1411D186725aEa"

        # Jingcheng
        # URL = "https://api.jingcheng.love/v1"
        # API_KEY = "sk-CPK90xN6naFR0lTuB8Da8e67Ab054cF4A5978cAa4dC6B445"
        return URL, API_KEY

```

## 2
```

    @app.route('/v1/chat/completions', methods=['POST'])
    @jwt_required()
    def chat_completions():
        user_id = get_jwt_identity()
        if not user_id:
            return jsonify({"message": "Unauthorized"}), 401
        
        user = User.query.get(user_id)
        if user is None:
            return jsonify({"message": "User not found"}), 404

        # Check if the user has enough balance
        if user.balance <= 0:
            return jsonify({"message": "Insufficient balance"}), 402

        url, api_key = OpenAIConfig.get_credentials()
        incoming_data = request.json
        
        # Set the correct headers based on the successful request example
        headers = {
            'Accept': 'text/event-stream',
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {api_key}'
        }

        print(f"relay data: {incoming_data}")
        print(f"header: {headers}")

        def generate():
            with requests.post(f'{url}/chat/completions', json=incoming_data, headers=headers, stream=True) as r:
                try:
                    r.raise_for_status()
                    for chunk in r.iter_content(chunk_size=None):  # Set chunk size as None to get data as it arrives
                        if chunk:  # filter out keep-alive new chunks
                            print(chunk.decode('utf-8'))  # Optional: for debugging purposes
                            yield chunk.decode('utf-8')
                except requests.exceptions.HTTPError as http_err:
                    # Prints the HTTP status code and text if an HTTP error occurs
                    error_message = f'HTTP error occurred: {http_err} - Status Code: {r.status_code} - Response: {r.text}'
                    print(error_message)  # Optional: for debugging purposes
                    yield f'Error: {error_message}'  # You may want to handle this more gracefully
                except requests.exceptions.RequestException as req_err:
                    # Prints the request exception message if a different RequestException occurs
                    error_message = f'Request error occurred: {req_err}'
                    print(error_message)  # Optional: for debugging purposes
                    yield f'Error: {error_message}'  # You may want to handle this more gracefully

        return Response(stream_with_context(generate()), content_type='text/event-stream')
```

Please:
1. calculate prompt tokens and response tokens, using tiktoken package,
2. find user, add to tokens used field.
3. Calculate the balance used for these tokens. deduct from user.balance.

```
User model:

# Define the User data-model.
class User(common.db.Model, UserMixin):
    id = common.db.Column(common.db.Integer, primary_key=True)

    # User authentication information
    username = common.db.Column(common.db.String(50), nullable=False, unique=True)
    password_hash = common.db.Column(common.db.String(255), nullable=False, server_default='')
    email = common.db.Column(common.db.String(255), nullable=False, unique=True)
    email_confirmed_at = common.db.Column(common.db.DateTime())

    # User information
    active = common.db.Column('is_active', common.db.Boolean(), nullable=False, server_default='0')

    # New fields for Corp association and admin flag
    corp_id = common.db.Column(common.db.Integer, nullable=True)  # Not enforced as a foreign key
    is_admin = common.db.Column(common.db.Boolean, default=False)

    # User balance and tokens used
    balance = common.db.Column(common.db.Float, nullable=False, default=0.0)
    tokens_used = common.db.Column(common.db.Integer, nullable=False, default=0)

```
Example of request data, 'incoming_data' in chat_completions():
```
{'messages': [{'role': 'system', 'content': '\nYou are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2021-09\nCurrent model: gpt-3.5-turbo\nCurrent time: 12/28/2023, 12:33:50 PM\nLatex inline: $x^2$ \nLatex block: $$e=mc^2$$\n\n'}, {'role': 'user', 'content': 'Hey\n'}], 'stream': True, 'model': 'gpt-3.5-turbo', 'temperature': 0.5, 'presence_penalty': 0, 'frequency_penalty': 0, 'top_p': 1}
header: {'Accept': 'text/event-stream', 'Content-Type': 'application/json', 'Authorization': 'Bearer sk-ILF01mttjbDHoF7pA01aB6690d3643B98c1411D186725aEa'}
data: {"id":"chatcmpl-8acNe19LvoM1XNw61AsQd9e7yn8VW","object":"chat.completion.chunk","created":1703738034,"model":"gpt-3.5-turbo-0613","system_fingerprint":null,"choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}
```

Example of chunk received from openai API. Please extract 'content' field and calculate the token used.
```
data: {"id":"chatcmpl-89CaD0agokRgV8feWNGi7jtR1eHwF","object":"chat.completion.chunk","model":"gpt-4-all","created":1703734700,"choices":[{"index":0,"delta":{"content":"化"},"finish_reason":null}]}

data: {"id":"chatcmpl-89CaD0agokRgV8feWNGi7jtR1eHwF","object":"chat.completion.chunk","model":"gpt-4-all","created":1703734700,"choices":[{"index":0,"delta":{"content":"这"},"finish_reason":null}]}
```

Rate calculation:
```
class OpenAIConfig:
    BASIC_RATE = 0.001 / 1000

    # Cost ratio for different models
    MODEL_RATIOS = {
        'gpt-3.5-turbo': 0.5,
        'gpt-4-1106-preview': 5,
    }

    @staticmethod
    def get_credentials():
        # These should be replaced with environment variables or a secure method to store credentials
        URL = "https://ai98.vip/v1"
        API_KEY = "sk-ILF01mttjbDHoF7pA01aB6690d3643B98c1411D186725aEa"

        # Jingcheng
        # URL = "https://api.jingcheng.love/v1"
        # API_KEY = "sk-CPK90xN6naFR0lTuB8Da8e67Ab054cF4A5978cAa4dC6B445"
        return URL, API_KEY

    @classmethod
    def get_model_cost_ratios(cls):
        return {model: cls.BASIC_RATE * ratio for model, ratio in cls.MODEL_RATIOS.items()}
```
Balance used = prompt token * model rate + response token * model rate



## 3

Please help me add a flask application-wide in memory dict cache, that stores user_id -> response token used.
For each chunk, count how many "data: " in this chunk, and add this num * 2 to this user's tokens used.

```

        def generate():
            with requests.post(f'{url}/chat/completions', json=incoming_data, headers=headers, stream=True) as r:
                try:
                    r.raise_for_status()
                    full_content = ''
                    for chunk in r.iter_content(chunk_size=None):  # Set chunk size as None to get data as it arrives
                        if chunk:  # filter out keep-alive new chunks
                            chunk_data = chunk.decode('utf-8')
                            print(f"- {chunk.decode('utf-8')}")  # Optional: for debugging purposes
                            full_content += chunk_data
                            yield chunk.decode('utf-8')
```

```
# app_common.py


def create_app(config_name):
    app = Flask(__name__)
    jwt = JWTManager(app)
```

## 4
Here's my sidebar.tsx
```

const BalanceModal: React.FC<BalanceModalProps> = ({ balance, tokensUsed }) => {
  return (
    <div className={styles.modalBackdrop}>
      <div className={styles.modal}>
        <h3>User Balance</h3>
        <p>Balance: {balance}</p>
        <p>Tokens used: {tokensUsed.toLocaleString()}</p>
      </div>
    </div>
  );
};

```
 I want to actually load the data from backend, api.easyai.codes/api/user/[id]
 Please help me use React best practice to do this.